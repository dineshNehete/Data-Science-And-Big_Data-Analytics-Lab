{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnZT3JLHbzAtHH+pT2/OWC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part-1\n",
        "> Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "\n"
      ],
      "metadata": {
        "id": "TGNkSvc1Z4a2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5ChXlsVMm5g"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"I woke up to the sound of birds singing outside. The sun filled the room with warmth and light. I brewed a fresh cup of coffee and savored its rich aroma. Immersed in work, time slipped away as I typed on my keyboard. A gentle breeze refreshed me during a break. I inhaled the scent of blooming flowers, feeling at peace.\"\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuRd-vRSYs5",
        "outputId": "3cec7d1d-5994-43f1-cf20-6eed3dd5b028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I woke up to the sound of birds singing outside. The sun filled the room with warmth and light. I brewed a fresh cup of coffee and savored its rich aroma. Immersed in work, time slipped away as I typed on my keyboard. A gentle breeze refreshed me during a break. I inhaled the scent of blooming flowers, feeling at peace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnUI8WN9SeiS",
        "outputId": "82b138a1-7e3e-4619-9e5d-5fd04d62e24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = sent_tokenize(corpus)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7gt2ZnESs-v",
        "outputId": "6006ada6-73e1-46c7-a968-494cea2b6d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I woke up to the sound of birds singing outside.', 'The sun filled the room with warmth and light.', 'I brewed a fresh cup of coffee and savored its rich aroma.', 'Immersed in work, time slipped away as I typed on my keyboard.', 'A gentle breeze refreshed me during a break.', 'I inhaled the scent of blooming flowers, feeling at peace.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words = word_tokenize(corpus)\n",
        "print(tokenized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7M-GFlVS_aM",
        "outputId": "89300a3b-ae90-4ae8-b0d6-0718eb736c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'woke', 'up', 'to', 'the', 'sound', 'of', 'birds', 'singing', 'outside', '.', 'The', 'sun', 'filled', 'the', 'room', 'with', 'warmth', 'and', 'light', '.', 'I', 'brewed', 'a', 'fresh', 'cup', 'of', 'coffee', 'and', 'savored', 'its', 'rich', 'aroma', '.', 'Immersed', 'in', 'work', ',', 'time', 'slipped', 'away', 'as', 'I', 'typed', 'on', 'my', 'keyboard', '.', 'A', 'gentle', 'breeze', 'refreshed', 'me', 'during', 'a', 'break', '.', 'I', 'inhaled', 'the', 'scent', 'of', 'blooming', 'flowers', ',', 'feeling', 'at', 'peace', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmed_words =[]\n",
        "\n",
        "for w in tokenized_words: \n",
        "  ps = PorterStemmer()\n",
        "  stem_word = ps.stem(w)\n",
        "  stemmed_words.append(stem_word)\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58RGQ9fPT8-a",
        "outputId": "150fdfa1-b45b-42ca-e445-9ce11f714c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'woke', 'up', 'to', 'the', 'sound', 'of', 'bird', 'sing', 'outsid', '.', 'the', 'sun', 'fill', 'the', 'room', 'with', 'warmth', 'and', 'light', '.', 'i', 'brew', 'a', 'fresh', 'cup', 'of', 'coffe', 'and', 'savor', 'it', 'rich', 'aroma', '.', 'immers', 'in', 'work', ',', 'time', 'slip', 'away', 'as', 'i', 'type', 'on', 'my', 'keyboard', '.', 'a', 'gentl', 'breez', 'refresh', 'me', 'dure', 'a', 'break', '.', 'i', 'inhal', 'the', 'scent', 'of', 'bloom', 'flower', ',', 'feel', 'at', 'peac', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "MBr1HnSGU8Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in tokenized_words])\n",
        "# print(type(lemmatized_output))\n",
        "print(lemmatized_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1RllOqyVnrx",
        "outputId": "c798089b-2465-4247-e463-c91a25b1ea04"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I woke up to the sound of bird singing outside . The sun filled the room with warmth and light . I brewed a fresh cup of coffee and savored it rich aroma . Immersed in work , time slipped away a I typed on my keyboard . A gentle breeze refreshed me during a break . I inhaled the scent of blooming flower , feeling at peace .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = []\n",
        "\n",
        "for i in tokenized_words:\n",
        "  lemmatized_word = lemmatizer.lemmatize(i)\n",
        "  lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oa0QeaVVxrL",
        "outputId": "fe0eb519-1d2a-43e9-eea5-412ff091196d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'woke', 'up', 'to', 'the', 'sound', 'of', 'bird', 'singing', 'outside', '.', 'The', 'sun', 'filled', 'the', 'room', 'with', 'warmth', 'and', 'light', '.', 'I', 'brewed', 'a', 'fresh', 'cup', 'of', 'coffee', 'and', 'savored', 'it', 'rich', 'aroma', '.', 'Immersed', 'in', 'work', ',', 'time', 'slipped', 'away', 'a', 'I', 'typed', 'on', 'my', 'keyboard', '.', 'A', 'gentle', 'breeze', 'refreshed', 'me', 'during', 'a', 'break', '.', 'I', 'inhaled', 'the', 'scent', 'of', 'blooming', 'flower', ',', 'feeling', 'at', 'peace', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "#CC: Coordinating Conjunction, CD: Cardinal Number, DT: Determiner, IN: Preposition, JJ: Adjective, MD: Modal, NN: Noun, NNS: Noun, Plural, NNP: Proper Noun, Singular, NNPS: Proper Noun, Plural\n",
        "#PRP: Personal Pronoun,  PRP$: Possessive Pronoun, RB: Adverb, TO: Infinitival \"to\",VB: Verb, Base Form, WP: Wh-Pronoun\n",
        "\n",
        "print(\"Parts Of Speech: \",nltk.pos_tag(tokenized_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C90P4RwbWQcY",
        "outputId": "dbfb0a41-2130-41dd-e1c4-c8e3e3edb9cb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts Of Speech:  [('I', 'PRP'), ('woke', 'VBD'), ('up', 'RB'), ('to', 'TO'), ('the', 'DT'), ('sound', 'NN'), ('of', 'IN'), ('birds', 'NNS'), ('singing', 'VBG'), ('outside', 'JJ'), ('.', '.'), ('The', 'DT'), ('sun', 'NN'), ('filled', 'VBD'), ('the', 'DT'), ('room', 'NN'), ('with', 'IN'), ('warmth', 'NN'), ('and', 'CC'), ('light', 'NN'), ('.', '.'), ('I', 'PRP'), ('brewed', 'VBD'), ('a', 'DT'), ('fresh', 'JJ'), ('cup', 'NN'), ('of', 'IN'), ('coffee', 'NN'), ('and', 'CC'), ('savored', 'VBD'), ('its', 'PRP$'), ('rich', 'JJ'), ('aroma', 'NN'), ('.', '.'), ('Immersed', 'VBN'), ('in', 'IN'), ('work', 'NN'), (',', ','), ('time', 'NN'), ('slipped', 'VBD'), ('away', 'RB'), ('as', 'IN'), ('I', 'PRP'), ('typed', 'VBD'), ('on', 'IN'), ('my', 'PRP$'), ('keyboard', 'NN'), ('.', '.'), ('A', 'DT'), ('gentle', 'JJ'), ('breeze', 'NN'), ('refreshed', 'VBD'), ('me', 'PRP'), ('during', 'IN'), ('a', 'DT'), ('break', 'NN'), ('.', '.'), ('I', 'PRP'), ('inhaled', 'VBD'), ('the', 'DT'), ('scent', 'NN'), ('of', 'IN'), ('blooming', 'VBG'), ('flowers', 'NNS'), (',', ','), ('feeling', 'VBG'), ('at', 'IN'), ('peace', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop Words:\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_in_english = stopwords.words('english')\n",
        "print(stop_words_in_english)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5DAA0O7Wo9T",
        "outputId": "3dfadb51-feb7-4322-d382-4b6e91f0f538"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in corpus.split() if word.lower() not in stop_words_in_english]\n",
        "new_corpus = \" \".join(words)\n",
        "print(new_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vMHo68LYexx",
        "outputId": "2fce5a64-c998-4cc2-b939-b406af0c3ef7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "woke sound birds singing outside. sun filled room warmth light. brewed fresh cup coffee savored rich aroma. Immersed work, time slipped away typed keyboard. gentle breeze refreshed break. inhaled scent blooming flowers, feeling peace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part - 2\n",
        "\n",
        ">  Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n",
        "\n"
      ],
      "metadata": {
        "id": "qKf3nHOzZ3R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_term_frequency(text):\n",
        "  text = text.lower()\n",
        "  tokens = text.split(\" \")\n",
        "\n",
        "  term_frequency_map = {}\n",
        "  text_size = len(document_text)\n",
        "\n",
        "  for token in tokens:\n",
        "      term_frequency_map[token] = term_frequency_map.get(token, 0) + 1\n",
        "      \n",
        "  return term_frequency_map"
      ],
      "metadata": {
        "id": "ZEYXYrDwalG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_text = \"This is a text from a sample document which is used for calculating the term frequency.\"\n",
        "tf = calculate_term_frequency(document_text)\n",
        "print(tf)\n",
        "#higher the term frequency more important is the word in the document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Mr0JJzb8vF",
        "outputId": "c7a21bbe-906a-486a-f7fb-98fbcc4a65ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 1, 'is': 2, 'a': 2, 'text': 1, 'from': 1, 'sample': 1, 'document': 1, 'which': 1, 'used': 1, 'for': 1, 'calculating': 1, 'the': 1, 'term': 1, 'frequency.': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IDF -> In how many documents does the query keyword appears\n",
        "# Calculated -> logarithm of ratio of number of documents in a collection to number of documents containing the given word.\n",
        "def calculate_inverse_document_frequency(documents):\n",
        "  tokenized_docs = []\n",
        "\n",
        "  #tokenizing the document text by converting to lower case and the splitting -> \"This is sent\" => [\"this\", \"is\", \"sent\"]\n",
        "  for doc in documents:\n",
        "    tokenized_docs.append(doc.lower().split())\n",
        "\n",
        "  #stores the frequency of every word occuring in all the documents\n",
        "  document_frequency = {}\n",
        "  for doc in tokenized_docs:\n",
        "    unique_words = set(doc)\n",
        "    for term in unique_words:\n",
        "      document_frequency[term] = document_frequency.get(term, 0) + 1\n",
        "    \n",
        "  n = len(tokenized_docs)\n",
        "  inverse_document_frequency = {}\n",
        "  # print(document_frequency)\n",
        "  print(\"-----------------------------------------\")\n",
        "  for term, df in document_frequency.items():\n",
        "    inverse_document_frequency[term] = math.log(n/ df)\n",
        "  return inverse_document_frequency\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "z3tsLqdvcDe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentList = [\n",
        "      \"This is the first document\",\n",
        "      \"This document is the second document\",\n",
        "      \"And this is the third one\",\n",
        "      \"Is this the first document?\"\n",
        "  ]\n",
        "\n",
        "idf_scores = calculate_inverse_document_frequency(documentList)\n",
        "for term, score in idf_scores.items():\n",
        "    print(f\"Term: {term}, IDF: {score}\")\n",
        "    print(\"-----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQHvLeR3fOXZ",
        "outputId": "be22d244-c3a7-47ec-c9b5-f57a94ee45ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------\n",
            "Term: document, IDF: 1.0986122886681098\n",
            "-----------------------------------------\n",
            "Term: is, IDF: 0.6931471805599453\n",
            "-----------------------------------------\n",
            "Term: this, IDF: 0.6931471805599453\n",
            "-----------------------------------------\n",
            "Term: the, IDF: 0.6931471805599453\n",
            "-----------------------------------------\n",
            "Term: first, IDF: 1.0986122886681098\n",
            "-----------------------------------------\n",
            "Term: second, IDF: 1.6094379124341003\n",
            "-----------------------------------------\n",
            "Term: one, IDF: 1.6094379124341003\n",
            "-----------------------------------------\n",
            "Term: third, IDF: 1.6094379124341003\n",
            "-----------------------------------------\n",
            "Term: and, IDF: 1.6094379124341003\n",
            "-----------------------------------------\n",
            "Term: document?, IDF: 1.6094379124341003\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAgIcjBlgVHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oyvzhy1YgT0I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}